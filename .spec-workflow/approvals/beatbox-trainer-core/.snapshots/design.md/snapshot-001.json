{
  "id": "snapshot_1762961200455_cog5k7awf",
  "approvalId": "approval_1762961200453_d6gck1tt6",
  "approvalTitle": "Design Document - Beatbox Trainer Core",
  "version": 1,
  "timestamp": "2025-11-12T15:26:40.455Z",
  "trigger": "initial",
  "status": "pending",
  "content": "# Design Document\n\n## Overview\n\nThe Beatbox Trainer Core implements a real-time rhythm training system using a 4-layer native architecture specifically designed for uncompromising audio performance. The system processes audio through a sophisticated pipeline: C++ Oboe handles low-latency I/O, Rust performs lock-free DSP analysis, and Flutter provides responsive UI feedback. This design eliminates the fundamental limitations of high-level audio frameworks (latency, jitter, GC pauses) while maintaining code safety through Rust's ownership model.\n\nThe architecture employs three concurrent threads with strict real-time constraints: an AudioThread (must complete within buffer duration), an AnalysisThread (processes DSP with no time bounds), and a UI Thread (React to results with < 100ms latency). Communication between threads uses lock-free SPSC queues with pre-allocated buffer pools, ensuring zero allocations in the audio path. This design delivers sample-accurate metronome generation (0 jitter) and sub-20ms end-to-end latency.\n\n## Steering Document Alignment\n\n### Technical Standards (tech.md)\n\n**4-Layer Stack Implementation**:\n- Layer 1 (C++ Oboe): Implemented via `oboe-rs` crate with full-duplex master-slave pattern\n- Layer 2 (Rust): Audio engine in `rust/src/`, three sub-modules (audio/, analysis/, calibration/)\n- Layer 3 (flutter_rust_bridge): Auto-generated bindings in `lib/bridge/api.dart` and `rust/src/api.rs`\n- Layer 4 (Dart/Flutter): UI screens in `lib/ui/screens/`, widgets in `lib/ui/widgets/`\n\n**Lock-Free Architecture**:\n- `rtrb::RingBuffer` for DATA_QUEUE and POOL_QUEUE\n- Dual-queue object pool pattern prevents allocations\n- Audio callback uses only `copy_from_slice()` and modulo arithmetic\n\n**Heuristic DSP Approach**:\n- `aus` crate for spectral centroid and ZCR extraction\n- `rustfft` for 256-sample (onset) and 1024-sample (classification) FFTs\n- Custom adaptive thresholding logic in `analysis/onset.rs`\n\n**JNI Initialization**:\n- Manual `JNI_OnLoad` implementation in `rust/src/lib.rs`\n- `MainActivity.kt` init block calls `System.loadLibrary(\"beatbox_trainer\")`\n- `ndk_context::initialize_android_context()` invoked before Oboe usage\n\n### Project Structure (structure.md)\n\n**Directory Organization**:\n```\nlib/\n├── main.dart                    # Entry point\n├── ui/\n│   ├── screens/\n│   │   ├── calibration_screen.dart\n│   │   └── training_screen.dart\n│   └── widgets/\n│       ├── classification_indicator.dart\n│       ├── timing_feedback.dart\n│       └── bpm_control.dart\n├── bridge/\n│   └── api.dart                 # flutter_rust_bridge generated\n└── models/\n    ├── classification_result.dart\n    ├── timing_feedback.dart\n    └── calibration_state.dart\n\nrust/src/\n├── lib.rs                       # JNI_OnLoad, crate root\n├── api.rs                       # Public API (@frb annotations)\n├── audio/\n│   ├── mod.rs\n│   ├── engine.rs                # AudioEngine struct, Oboe callbacks\n│   ├── metronome.rs             # Sample-accurate click generation\n│   └── buffer_pool.rs           # SPSC queue + pool pattern\n├── analysis/\n│   ├── mod.rs\n│   ├── onset.rs                 # Spectral flux onset detection\n│   ├── features.rs              # DSP feature extraction\n│   ├── classifier.rs            # Heuristic classification rules\n│   └── quantizer.rs             # Timing quantization logic\n└── calibration/\n    ├── mod.rs\n    ├── state.rs                 # CalibrationState struct\n    └── procedure.rs             # Calibration workflow\n```\n\n**Naming Conventions**:\n- Rust: `snake_case` modules, `PascalCase` structs, `UPPER_SNAKE_CASE` constants\n- Dart: `snake_case` files, `PascalCase` classes, `camelCase` methods\n- Kotlin: `PascalCase` classes, `camelCase` methods\n\n**Real-Time Safety Checklist Applied**:\n- No heap allocations in `audio/engine.rs` callbacks\n- No locks in `audio/` module\n- All buffers pre-allocated in `buffer_pool.rs`\n\n## Code Reuse Analysis\n\n### Existing Components to Leverage\n\n**From Flutter Ecosystem**:\n- **Material Design widgets**: `Scaffold`, `AppBar`, `FloatingActionButton` for base UI structure\n- **StreamBuilder**: Reactive UI updates from Rust Stream\n- **Slider**: BPM control widget (40-240 range)\n\n**From Rust Crate Ecosystem**:\n- **oboe-rs** (v0.6): Complete Oboe bindings, no custom wrapper needed\n- **rtrb** (v0.3): Production-ready SPSC queue, proven in audio applications\n- **rustfft** (v6): Fastest pure-Rust FFT, no need for custom implementation\n- **aus** or **estratto**: Feature extraction functions already implemented\n\n**From Android SDK**:\n- **Permissions API**: Runtime RECORD_AUDIO permission flow (Flutter plugin: `permission_handler`)\n- **Audio Manager**: Query device latency characteristics via platform channel\n\n### Integration Points\n\n**Flutter ↔ Rust Bridge**:\n- `flutter_rust_bridge_codegen` generates bidirectional bindings automatically\n- Rust functions annotated with `#[flutter_rust_bridge::frb]` become callable from Dart\n- Rust `Stream<T>` becomes Dart `Stream<T>` (async iteration in UI)\n\n**Oboe ↔ Android Audio Hardware**:\n- Oboe automatically selects best backend (AAudio on Android 8.1+, OpenSL ES fallback)\n- No manual backend selection required\n\n**Audio Thread ↔ Analysis Thread**:\n- `rtrb::Producer`/`Consumer` split ownership (Send + Sync bounds)\n- Audio thread owns `Producer`, Analysis thread owns `Consumer`\n\n## Architecture\n\n### Modular Design Principles\n\n**Single File Responsibility**:\n- `audio/engine.rs`: Audio I/O lifecycle only (no DSP logic)\n- `analysis/onset.rs`: Onset detection only (no classification)\n- `calibration/state.rs`: Threshold storage only (no measurement)\n\n**Component Isolation**:\n- Each Rust module is self-contained (no circular dependencies)\n- Each Dart screen is independent (no shared mutable state)\n\n**Service Layer Separation**:\n- Audio layer provides raw audio streams\n- Analysis layer provides DSP results\n- UI layer presents results (no business logic)\n\n**Utility Modularity**:\n- `audio/buffer_pool.rs`: Reusable lock-free pool pattern\n- `analysis/features.rs`: Pure functions for DSP calculations\n\n### System Architecture Diagram\n\n```mermaid\ngraph TD\n    subgraph \"Layer 4: Dart/Flutter UI\"\n        UI[TrainingScreen]\n        Calibration[CalibrationScreen]\n        Indicator[ClassificationIndicator]\n        Feedback[TimingFeedback]\n        BPM[BPMControl]\n    end\n\n    subgraph \"Layer 3: flutter_rust_bridge\"\n        API[api.dart / api.rs]\n        Stream[Stream<ClassificationResult>]\n    end\n\n    subgraph \"Layer 2: Rust Audio Engine\"\n        AudioEngine[AudioEngine]\n        MetronomeGen[Metronome Generator]\n        BufferPool[BufferPool<br/>DATA_QUEUE + POOL_QUEUE]\n        OnsetDetector[OnsetDetector<br/>256-sample FFT]\n        FeatureExtractor[FeatureExtractor<br/>1024-sample FFT]\n        Classifier[Classifier<br/>Heuristic Rules]\n        Quantizer[Quantizer<br/>Timing Calculation]\n        CalibState[CalibrationState]\n    end\n\n    subgraph \"Layer 1: C++ Oboe (via oboe-rs)\"\n        OboeOutput[Output Callback<br/>Master]\n        OboeInput[Input Stream<br/>Slave]\n        Speaker[Speaker<br/>Hardware]\n        Mic[Microphone<br/>Hardware]\n    end\n\n    %% Layer 4 → Layer 3\n    UI -->|start_audio<br/>set_bpm| API\n    Calibration -->|calibrate| API\n    BPM -->|set_bpm| API\n    API -->|classification_stream| Stream\n    Stream -->|BeatboxHit| Indicator\n    Stream -->|TimingFeedback| Feedback\n\n    %% Layer 3 → Layer 2\n    API -->|Rust function calls| AudioEngine\n    AudioEngine -->|spawn| OnsetDetector\n    Stream -.->|tokio channel| Classifier\n\n    %% Layer 2 Audio Path\n    AudioEngine -->|AudioOutputCallback| OboeOutput\n    AudioEngine -->|AudioInputCallback| OboeInput\n    OboeOutput -->|mix samples| MetronomeGen\n    OboeInput -->|copy_from_slice| BufferPool\n    BufferPool -->|pop buffer| OnsetDetector\n    OnsetDetector -->|t_onset| FeatureExtractor\n    FeatureExtractor -->|Features| Classifier\n    Classifier -->|BeatboxHit| Quantizer\n    Quantizer -->|TimingFeedback| Stream\n    BufferPool -->|return buffer| BufferPool\n    CalibState -.->|thresholds| Classifier\n\n    %% Layer 1 Hardware\n    OboeOutput -->|audio frames| Speaker\n    Mic -->|audio frames| OboeInput\n\n    style UI fill:#e1f5fe\n    style Calibration fill:#e1f5fe\n    style Indicator fill:#e1f5fe\n    style Feedback fill:#e1f5fe\n    style BPM fill:#e1f5fe\n    style API fill:#fff9c4\n    style Stream fill:#fff9c4\n    style AudioEngine fill:#c8e6c9\n    style BufferPool fill:#ffccbc\n    style OnsetDetector fill:#b2dfdb\n    style Classifier fill:#b2dfdb\n    style OboeOutput fill:#f8bbd0\n    style OboeInput fill:#f8bbd0\n```\n\n### Thread Architecture Diagram\n\n```mermaid\nsequenceDiagram\n    participant UI as UI Thread<br/>(Dart)\n    participant Audio as AudioThread<br/>(Oboe Callback)\n    participant Analysis as AnalysisThread<br/>(Rust)\n\n    Note over UI: User taps \"Start\"\n    UI->>Audio: start_audio(120 BPM)\n    Audio->>Audio: Initialize Oboe streams\n    Audio->>Analysis: Spawn analysis thread\n\n    loop Every ~10ms (buffer callback)\n        Note over Audio: Output Callback (Master)\n        Audio->>Audio: Generate metronome click\n        Audio->>Audio: Mix click into output buffer\n        Audio->>Audio: Non-blocking read from input\n        Audio->>Audio: frame_counter += buffer_size\n\n        Note over Audio: Input Processing\n        Audio->>Audio: pool_queue.pop() → buffer\n        Audio->>Audio: buffer.copy_from_slice(mic_data)\n        Audio->>Analysis: data_queue.push(buffer)\n    end\n\n    loop Analysis Loop (non-blocking)\n        Analysis->>Analysis: data_queue.pop() → buffer\n        Note over Analysis: Onset Detection (256 FFT)\n        Analysis->>Analysis: Spectral flux calculation\n        Analysis->>Analysis: Adaptive threshold peak pick\n\n        alt Onset Detected\n            Note over Analysis: Classification (1024 FFT)\n            Analysis->>Analysis: Extract centroid, ZCR\n            Analysis->>Analysis: Apply heuristic rules\n            Analysis->>Analysis: Quantize timing vs metronome\n            Analysis->>UI: Stream.push(ClassificationResult)\n        end\n\n        Analysis->>Audio: pool_queue.push(buffer)\n    end\n\n    UI->>UI: StreamBuilder rebuilds\n    UI->>UI: Update classification indicator\n    UI->>UI: Update timing feedback\n```\n\n## Components and Interfaces\n\n### Component 1: AudioEngine (Rust)\n\n**Purpose**: Manages Oboe audio streams, metronome generation, and lock-free data transfer to analysis thread.\n\n**File**: `rust/src/audio/engine.rs`\n\n**Interfaces**:\n```rust\npub struct AudioEngine {\n    output_stream: Option<AudioStreamAsync<Output>>,\n    input_stream: Option<AudioStreamAsync<Input>>,\n    sample_rate: u32,\n    frame_counter: Arc<AtomicU64>,\n    bpm: Arc<AtomicU32>,\n    data_producer: Producer<Vec<f32>>,\n    pool_consumer: Consumer<Vec<f32>>,\n}\n\nimpl AudioEngine {\n    pub fn new(bpm: u32) -> Result<Self, AudioError>;\n    pub fn start(&mut self) -> Result<(), AudioError>;\n    pub fn stop(&mut self) -> Result<(), AudioError>;\n    pub fn set_bpm(&self, bpm: u32);\n}\n\n// Oboe callbacks\nimpl AudioOutputCallback for AudioEngine {\n    fn on_audio_ready(&mut self, stream: &mut AudioOutputStreamSafe, audio_data: &mut [f32]) -> DataCallbackResult;\n}\n```\n\n**Dependencies**:\n- `oboe` crate: Audio stream management\n- `audio/metronome.rs`: Click generation function\n- `audio/buffer_pool.rs`: SPSC queue setup\n- `std::sync::atomic`: Atomic BPM and frame counter\n\n**Reuses**:\n- `oboe-rs` FullDuplexStream pattern (documented in Oboe samples)\n- `rtrb` lock-free queue (standard pattern in audio applications)\n\n**Real-Time Safety**:\n- `on_audio_ready` uses only:\n  - Atomic loads (`bpm.load(Ordering::Relaxed)`)\n  - Modulo arithmetic (`frame_counter % samples_per_beat`)\n  - `pool_consumer.pop()` (lock-free, returns `Err` if empty)\n  - `data_producer.push()` (lock-free, returns `Err` if full)\n  - `copy_from_slice()` (no allocations)\n\n### Component 2: BufferPool (Rust)\n\n**Purpose**: Implements lock-free buffer pool pattern using dual SPSC queues to eliminate allocations in audio thread.\n\n**File**: `rust/src/audio/buffer_pool.rs`\n\n**Interfaces**:\n```rust\npub struct BufferPool {\n    data_queue: (Producer<Vec<f32>>, Consumer<Vec<f32>>),\n    pool_queue: (Producer<Vec<f32>>, Consumer<Vec<f32>>),\n}\n\nimpl BufferPool {\n    pub fn new(buffer_count: usize, buffer_size: usize) -> Self;\n    pub fn split(self) -> (\n        (Producer<Vec<f32>>, Consumer<Vec<f32>>),  // DATA_QUEUE\n        (Producer<Vec<f32>>, Consumer<Vec<f32>>),  // POOL_QUEUE\n    );\n}\n```\n\n**Dependencies**:\n- `rtrb` crate: Lock-free SPSC ring buffers\n\n**Reuses**:\n- Standard object pool pattern from game development and audio processing\n\n**Initialization**:\n```rust\nlet pool = BufferPool::new(16, 2048);  // 16 buffers × 2048 samples\nlet ((data_prod, data_cons), (pool_prod, pool_cons)) = pool.split();\n\n// Pre-fill pool with empty buffers\nfor _ in 0..16 {\n    pool_prod.push(vec![0.0f32; 2048]).unwrap();\n}\n\n// Give data_prod to AudioEngine\n// Give data_cons to AnalysisThread\n// Give pool_prod to AnalysisThread\n// Give pool_cons to AudioEngine\n```\n\n### Component 3: OnsetDetector (Rust)\n\n**Purpose**: Detects sound onset timestamps using spectral flux algorithm with adaptive thresholding.\n\n**File**: `rust/src/analysis/onset.rs`\n\n**Interfaces**:\n```rust\npub struct OnsetDetector {\n    fft_planner: Arc<Mutex<FftPlanner<f32>>>,  // Locked ONLY in Analysis thread\n    prev_spectrum: Vec<f32>,\n    flux_signal: VecDeque<f32>,\n    sample_rate: u32,\n    window_size: usize,  // 256 samples\n}\n\nimpl OnsetDetector {\n    pub fn new(sample_rate: u32) -> Self;\n    pub fn process(&mut self, audio: &[f32]) -> Vec<u64> {\n        // Returns onset timestamps (in sample count since engine start)\n    }\n\n    fn compute_spectral_flux(&self, spectrum: &[f32]) -> f32;\n    fn adaptive_threshold(&self, window_size: usize) -> f32;\n    fn pick_peaks(&self, signal: &[f32], threshold: f32) -> Vec<usize>;\n}\n```\n\n**Dependencies**:\n- `rustfft` or `microfft`: FFT computation\n- `std::collections::VecDeque`: Circular buffer for flux signal\n\n**Reuses**:\n- Spectral flux algorithm (standard in MIR literature)\n- Adaptive thresholding (median + offset pattern)\n\n**Algorithm**:\n1. Compute 256-point FFT with 75% overlap (hop = 64 samples)\n2. Calculate magnitude spectrum: `|FFT[k]|`\n3. Compute positive difference from previous frame: `SF[k] = max(0, |FFT_t[k]| - |FFT_(t-1)[k]|)`\n4. Sum across frequency bins: `flux_t = Σ SF[k]`\n5. Apply adaptive threshold: `threshold_t = median(flux[t-50:t+50]) + 0.1`\n6. Peak pick: Find local maxima where `flux_t > threshold_t`\n\n### Component 4: FeatureExtractor (Rust)\n\n**Purpose**: Extracts DSP features (spectral centroid, ZCR, flatness, rolloff) from onset windows for classification.\n\n**File**: `rust/src/analysis/features.rs`\n\n**Interfaces**:\n```rust\npub struct Features {\n    pub centroid: f32,           // Hz (spectral brightness)\n    pub zcr: f32,                // 0-1 (noisiness)\n    pub flatness: f32,           // 0-1 (tonality vs noise) [Level 2]\n    pub rolloff: f32,            // Hz (85% energy threshold) [Level 2]\n    pub decay_time_ms: f32,      // ms (envelope decay) [Level 2]\n}\n\npub struct FeatureExtractor {\n    fft_planner: Arc<Mutex<FftPlanner<f32>>>,\n    sample_rate: u32,\n    window_size: usize,  // 1024 samples\n}\n\nimpl FeatureExtractor {\n    pub fn new(sample_rate: u32) -> Self;\n    pub fn extract(&mut self, audio_window: &[f32]) -> Features;\n\n    fn compute_centroid(spectrum: &[f32], sample_rate: u32) -> f32;\n    fn compute_zcr(audio: &[f32]) -> f32;\n    fn compute_flatness(spectrum: &[f32]) -> f32;\n    fn compute_rolloff(spectrum: &[f32], sample_rate: u32, threshold: f32) -> f32;\n    fn compute_decay_time(audio: &[f32], sample_rate: u32) -> f32;\n}\n```\n\n**Dependencies**:\n- `rustfft`: 1024-point FFT for classification\n- `aus` or `estratto`: Pre-implemented feature functions (optional, can be custom)\n\n**Reuses**:\n- Standard MIR feature definitions (centroid = weighted mean of spectrum)\n- ZCR formula from audio textbooks\n\n**Feature Formulas**:\n- **Centroid**: `Σ(f_i × |X[i]|) / Σ|X[i]|` where `f_i = i × (sample_rate / FFT_size)`\n- **ZCR**: `(1/N) Σ |sign(x[n]) - sign(x[n-1])|`\n- **Flatness**: `exp((1/N) Σ log|X[k]|) / ((1/N) Σ |X[k]|)` (geometric mean / arithmetic mean)\n- **Rolloff**: Frequency where `Σ_{k=0}^{k_r} |X[k]| = 0.85 × Σ_{k=0}^{N} |X[k]|`\n- **Decay Time**: Fit exponential to RMS envelope, measure time to -20dB\n\n### Component 5: Classifier (Rust)\n\n**Purpose**: Applies heuristic rules to classify sounds as KICK/SNARE/HIHAT/UNKNOWN based on calibrated thresholds.\n\n**File**: `rust/src/analysis/classifier.rs`\n\n**Interfaces**:\n```rust\n#[derive(Debug, Clone, PartialEq)]\npub enum BeatboxHit {\n    Kick,\n    Snare,\n    HiHat,\n    Unknown,\n    // Level 2 subcategories:\n    ClosedHiHat,\n    OpenHiHat,\n    KSnare,\n}\n\npub struct Classifier {\n    level: ClassificationLevel,\n    calibration: Arc<RwLock<CalibrationState>>,  // Read-only in analysis thread\n}\n\nimpl Classifier {\n    pub fn new(calibration: Arc<RwLock<CalibrationState>>) -> Self;\n    pub fn classify(&self, features: &Features) -> BeatboxHit;\n\n    fn classify_level1(&self, features: &Features) -> BeatboxHit;\n    fn classify_level2(&self, features: &Features) -> BeatboxHit;\n}\n```\n\n**Dependencies**:\n- `calibration::CalibrationState`: Threshold values\n\n**Reuses**:\n- Decision tree pattern from ML literature (but hardcoded, not learned)\n\n**Level 1 Rules** (from requirements):\n```rust\nfn classify_level1(&self, features: &Features) -> BeatboxHit {\n    let cal = self.calibration.read().unwrap();\n\n    if features.centroid < cal.t_kick_centroid && features.zcr < cal.t_kick_zcr {\n        BeatboxHit::Kick\n    } else if features.centroid < cal.t_snare_centroid {\n        BeatboxHit::Snare\n    } else if features.centroid >= cal.t_snare_centroid && features.zcr > cal.t_hihat_zcr {\n        BeatboxHit::HiHat\n    } else {\n        BeatboxHit::Unknown\n    }\n}\n```\n\n**Level 2 Rules**:\n- Use additional features (flatness, decay_time) to split broad categories into subcategories\n- Example: HiHat → ClosedHiHat (decay < 50ms) or OpenHiHat (decay > 150ms)\n\n### Component 6: Quantizer (Rust)\n\n**Purpose**: Calculates timing error between onset and metronome grid, classifies as ON_TIME/EARLY/LATE.\n\n**File**: `rust/src/analysis/quantizer.rs`\n\n**Interfaces**:\n```rust\n#[derive(Debug, Clone)]\npub enum TimingClassification {\n    OnTime,\n    Early,\n    Late,\n}\n\n#[derive(Debug, Clone)]\npub struct TimingFeedback {\n    pub classification: TimingClassification,\n    pub error_ms: f32,\n}\n\npub struct Quantizer {\n    frame_counter: Arc<AtomicU64>,  // Shared with AudioEngine\n    bpm: Arc<AtomicU32>,            // Shared with AudioEngine\n    sample_rate: u32,\n}\n\nimpl Quantizer {\n    pub fn new(frame_counter: Arc<AtomicU64>, bpm: Arc<AtomicU32>, sample_rate: u32) -> Self;\n    pub fn quantize(&self, onset_timestamp: u64) -> TimingFeedback;\n}\n```\n\n**Dependencies**:\n- Shared atomic references to `frame_counter` and `bpm` from AudioEngine\n\n**Reuses**:\n- Standard phase vocoder quantization logic\n\n**Algorithm**:\n```rust\npub fn quantize(&self, onset_timestamp: u64) -> TimingFeedback {\n    let bpm = self.bpm.load(Ordering::Relaxed);\n    let samples_per_beat = (self.sample_rate as f64 * 60.0) / bpm as f64;\n\n    let beat_error = (onset_timestamp as f64) % samples_per_beat;\n    let error_ms = (beat_error / self.sample_rate as f64) * 1000.0;\n\n    let beat_period_ms = (60.0 / bpm as f64) * 1000.0;\n    let tolerance_ms = 50.0;\n\n    let classification = if error_ms < tolerance_ms {\n        TimingClassification::OnTime\n    } else if error_ms > (beat_period_ms - tolerance_ms) {\n        TimingClassification::Early\n    } else {\n        TimingClassification::Late\n    };\n\n    TimingFeedback { classification, error_ms: error_ms as f32 }\n}\n```\n\n### Component 7: CalibrationState (Rust)\n\n**Purpose**: Stores user-calibrated thresholds for classification.\n\n**File**: `rust/src/calibration/state.rs`\n\n**Interfaces**:\n```rust\n#[derive(Debug, Clone)]\npub struct CalibrationState {\n    pub t_kick_centroid: f32,    // Hz\n    pub t_kick_zcr: f32,          // 0-1\n    pub t_snare_centroid: f32,    // Hz\n    pub t_hihat_zcr: f32,         // 0-1\n    pub is_calibrated: bool,\n}\n\nimpl CalibrationState {\n    pub fn new_default() -> Self {\n        // Default thresholds (before calibration)\n        Self {\n            t_kick_centroid: 1500.0,\n            t_kick_zcr: 0.1,\n            t_snare_centroid: 4000.0,\n            t_hihat_zcr: 0.3,\n            is_calibrated: false,\n        }\n    }\n\n    pub fn from_samples(kick_samples: &[Features], snare_samples: &[Features], hihat_samples: &[Features]) -> Self;\n}\n```\n\n**Dependencies**: None (pure data struct)\n\n**Reuses**: Standard statistics (mean calculation)\n\n**Calibration Algorithm**:\n```rust\npub fn from_samples(kick: &[Features], snare: &[Features], hihat: &[Features]) -> Self {\n    let mean_kick_centroid = kick.iter().map(|f| f.centroid).sum::<f32>() / kick.len() as f32;\n    let mean_kick_zcr = kick.iter().map(|f| f.zcr).sum::<f32>() / kick.len() as f32;\n\n    Self {\n        t_kick_centroid: mean_kick_centroid * 1.2,  // 20% margin\n        t_kick_zcr: mean_kick_zcr * 1.2,\n        t_snare_centroid: /* similar calculation */,\n        t_hihat_zcr: /* similar calculation */,\n        is_calibrated: true,\n    }\n}\n```\n\n### Component 8: TrainingScreen (Dart/Flutter)\n\n**Purpose**: Main training UI with real-time classification and timing feedback display.\n\n**File**: `lib/ui/screens/training_screen.dart`\n\n**Interfaces**:\n```dart\nclass TrainingScreen extends StatefulWidget {\n  @override\n  _TrainingScreenState createState() => _TrainingScreenState();\n}\n\nclass _TrainingScreenState extends State<TrainingScreen> {\n  late Stream<ClassificationResult> _classificationStream;\n  bool _isTraining = false;\n  int _currentBpm = 120;\n\n  void _startTraining() async {\n    await api.startAudio(bpm: _currentBpm);\n    setState(() {\n      _isTraining = true;\n      _classificationStream = api.classificationStream();\n    });\n  }\n\n  void _stopTraining() async {\n    await api.stopAudio();\n    setState(() => _isTraining = false);\n  }\n\n  void _setBpm(int bpm) {\n    setState(() => _currentBpm = bpm);\n    api.setBpm(bpm: bpm);\n  }\n}\n```\n\n**Dependencies**:\n- `lib/bridge/api.dart`: flutter_rust_bridge generated API\n- `lib/ui/widgets/classification_indicator.dart`\n- `lib/ui/widgets/timing_feedback.dart`\n- `lib/ui/widgets/bpm_control.dart`\n\n**Reuses**:\n- Flutter Material Design: `Scaffold`, `AppBar`, `FloatingActionButton`\n- `StreamBuilder` for reactive UI updates\n\n**Widget Tree**:\n```dart\nScaffold(\n  appBar: AppBar(title: Text('Beatbox Trainer')),\n  body: Column([\n    BPMControl(currentBpm: _currentBpm, onChanged: _setBpm),\n    StreamBuilder<ClassificationResult>(\n      stream: _classificationStream,\n      builder: (context, snapshot) {\n        if (!snapshot.hasData) return ClassificationIndicator.idle();\n        return ClassificationIndicator(result: snapshot.data!);\n      },\n    ),\n    StreamBuilder<ClassificationResult>(\n      stream: _classificationStream,\n      builder: (context, snapshot) {\n        if (!snapshot.hasData) return TimingFeedback.idle();\n        return TimingFeedback(result: snapshot.data!);\n      },\n    ),\n  ]),\n  floatingActionButton: FloatingActionButton(\n    onPressed: _isTraining ? _stopTraining : _startTraining,\n    child: Icon(_isTraining ? Icons.stop : Icons.play_arrow),\n  ),\n)\n```\n\n### Component 9: CalibrationScreen (Dart/Flutter)\n\n**Purpose**: Guides user through calibration process (10 samples × 3 sounds = 30 samples).\n\n**File**: `lib/ui/screens/calibration_screen.dart`\n\n**Interfaces**:\n```dart\nenum CalibrationSound { kick, snare, hihat }\n\nclass CalibrationScreen extends StatefulWidget {\n  @override\n  _CalibrationScreenState createState() => _CalibrationScreenState();\n}\n\nclass _CalibrationScreenState extends State<CalibrationScreen> {\n  CalibrationSound _currentSound = CalibrationSound.kick;\n  int _samplesCollected = 0;\n  final int _samplesRequired = 10;\n\n  void _startCalibration() async {\n    await api.startCalibration(sound: _currentSound);\n    // Listen to calibration progress stream\n  }\n\n  void _onSampleCollected() {\n    setState(() {\n      _samplesCollected++;\n      if (_samplesCollected >= _samplesRequired) {\n        _nextSound();\n      }\n    });\n  }\n\n  void _nextSound() {\n    if (_currentSound == CalibrationSound.kick) {\n      setState(() {\n        _currentSound = CalibrationSound.snare;\n        _samplesCollected = 0;\n      });\n    } else if (_currentSound == CalibrationSound.snare) {\n      setState(() {\n        _currentSound = CalibrationSound.hihat;\n        _samplesCollected = 0;\n      });\n    } else {\n      _finishCalibration();\n    }\n  }\n\n  void _finishCalibration() async {\n    await api.finishCalibration();\n    Navigator.pop(context);\n  }\n}\n```\n\n**Dependencies**:\n- `lib/bridge/api.dart`: Calibration functions\n- Flutter Material: Progress indicators, instructions text\n\n## Data Models\n\n### Model 1: ClassificationResult (Rust ↔ Dart)\n\n**Purpose**: Carries classification and timing feedback from Rust to Dart UI.\n\n**Rust Definition** (`rust/src/api.rs`):\n```rust\n#[derive(Debug, Clone)]\npub struct ClassificationResult {\n    pub sound: BeatboxHit,\n    pub timing: TimingFeedback,\n    pub timestamp_ms: u64,  // For logging/debugging\n}\n```\n\n**Dart Definition** (`lib/models/classification_result.dart`, auto-generated):\n```dart\nclass ClassificationResult {\n  final BeatboxHit sound;\n  final TimingFeedback timing;\n  final int timestampMs;\n\n  ClassificationResult({\n    required this.sound,\n    required this.timing,\n    required this.timestampMs,\n  });\n}\n\nenum BeatboxHit {\n  kick,\n  snare,\n  hiHat,\n  unknown,\n}\n```\n\n### Model 2: TimingFeedback (Rust ↔ Dart)\n\n**Rust Definition**:\n```rust\n#[derive(Debug, Clone)]\npub struct TimingFeedback {\n    pub classification: TimingClassification,\n    pub error_ms: f32,\n}\n\n#[derive(Debug, Clone, PartialEq)]\npub enum TimingClassification {\n    OnTime,\n    Early,\n    Late,\n}\n```\n\n**Dart Definition** (auto-generated):\n```dart\nclass TimingFeedback {\n  final TimingClassification classification;\n  final double errorMs;\n}\n\nenum TimingClassification {\n  onTime,\n  early,\n  late,\n}\n```\n\n### Model 3: CalibrationProgress (Rust ↔ Dart)\n\n**Purpose**: Reports calibration progress to UI.\n\n**Rust Definition**:\n```rust\n#[derive(Debug, Clone)]\npub struct CalibrationProgress {\n    pub current_sound: CalibrationSound,\n    pub samples_collected: usize,\n    pub samples_required: usize,\n}\n\n#[derive(Debug, Clone, PartialEq)]\npub enum CalibrationSound {\n    Kick,\n    Snare,\n    HiHat,\n}\n```\n\n**Dart Definition** (auto-generated):\n```dart\nclass CalibrationProgress {\n  final CalibrationSound currentSound;\n  final int samplesCollected;\n  final int samplesRequired;\n}\n\nenum CalibrationSound {\n  kick,\n  snare,\n  hiHat,\n}\n```\n\n## Error Handling\n\n### Error Scenario 1: Oboe Stream Initialization Fails\n\n**Description**: Device does not support low-latency audio (e.g., very old Android device or emulator without audio HAL).\n\n**Rust Handling**:\n```rust\nimpl AudioEngine {\n    pub fn start(&mut self) -> Result<(), AudioError> {\n        let output_stream = AudioStreamBuilder::default()\n            .set_performance_mode(PerformanceMode::LowLatency)\n            .set_usage(Usage::Game)  // Prioritizes latency\n            .set_sharing_mode(SharingMode::Exclusive)\n            .open_stream()?;  // Returns Err(oboe::Error) if fails\n\n        if output_stream.get_buffer_size_in_frames() > 1024 {\n            log::warn!(\"High buffer size detected: {} frames\", output_stream.get_buffer_size_in_frames());\n        }\n\n        self.output_stream = Some(output_stream);\n        Ok(())\n    }\n}\n\n#[derive(Debug)]\npub enum AudioError {\n    OboeError(oboe::Error),\n    DeviceNotSupported(String),\n}\n```\n\n**Dart Handling**:\n```dart\ntry {\n  await api.startAudio(bpm: 120);\n} catch (e) {\n  showDialog(\n    context: context,\n    builder: (context) => AlertDialog(\n      title: Text('Audio Error'),\n      content: Text('Failed to start audio engine: $e\\nTry closing other audio apps.'),\n      actions: [\n        TextButton(\n          onPressed: () => Navigator.pop(context),\n          child: Text('OK'),\n        ),\n      ],\n    ),\n  );\n}\n```\n\n**User Impact**: Error dialog with actionable advice (close other apps, restart device).\n\n### Error Scenario 2: Microphone Permission Denied\n\n**Description**: User denies RECORD_AUDIO permission on Android 6.0+.\n\n**Dart Handling** (using `permission_handler` plugin):\n```dart\nimport 'package:permission_handler/permission_handler.dart';\n\nFuture<void> _requestMicrophonePermission() async {\n  final status = await Permission.microphone.request();\n\n  if (status.isGranted) {\n    _startTraining();\n  } else if (status.isDenied) {\n    showDialog(\n      context: context,\n      builder: (context) => AlertDialog(\n        title: Text('Microphone Permission Required'),\n        content: Text('This app needs microphone access to detect your beatbox sounds.'),\n        actions: [\n          TextButton(onPressed: () => Navigator.pop(context), child: Text('Cancel')),\n          TextButton(onPressed: () => openAppSettings(), child: Text('Settings')),\n        ],\n      ),\n    );\n  } else if (status.isPermanentlyDenied) {\n    openAppSettings();\n  }\n}\n```\n\n**User Impact**: Clear explanation of why permission is needed, direct link to app settings.\n\n### Error Scenario 3: Calibration Sample Rejected (Invalid Input)\n\n**Description**: User produces sound outside expected range (e.g., whistling instead of beatboxing).\n\n**Rust Handling**:\n```rust\nimpl CalibrationProcedure {\n    pub fn add_sample(&mut self, features: Features) -> Result<(), CalibrationError> {\n        // Validate sample\n        if features.centroid > 20000.0 {\n            return Err(CalibrationError::InvalidSample(\"Frequency too high (> 20kHz)\"));\n        }\n        if features.centroid < 50.0 {\n            return Err(CalibrationError::InvalidSample(\"Frequency too low (< 50Hz)\"));\n        }\n\n        self.samples.push(features);\n        Ok(())\n    }\n}\n\n#[derive(Debug)]\npub enum CalibrationError {\n    InvalidSample(&'static str),\n    InsufficientSamples,\n}\n```\n\n**Dart Handling**:\n```dart\nStreamBuilder<CalibrationProgress>(\n  stream: api.calibrationStream(),\n  builder: (context, snapshot) {\n    if (snapshot.hasError) {\n      return Text(\n        'Invalid sample detected. Please try again.',\n        style: TextStyle(color: Colors.red),\n      );\n    }\n    return Text('Sample ${snapshot.data?.samplesCollected ?? 0}/10');\n  },\n)\n```\n\n**User Impact**: Immediate feedback that sample was rejected, prompt to retry.\n\n### Error Scenario 4: Buffer Pool Exhaustion (Processing Bottleneck)\n\n**Description**: Analysis thread cannot keep up with audio thread (CPU overload), causing pool_queue to empty.\n\n**Rust Handling**:\n```rust\nimpl AudioOutputCallback for AudioEngine {\n    fn on_audio_ready(&mut self, stream: &mut AudioOutputStreamSafe, audio_data: &mut [f32]) -> DataCallbackResult {\n        // Try to get buffer from pool\n        let mut buffer = match self.pool_consumer.pop() {\n            Ok(buf) => buf,\n            Err(_) => {\n                // Pool is empty - drop this audio frame\n                log::warn!(\"Buffer pool exhausted, dropping audio frame\");\n                return DataCallbackResult::Continue;\n            }\n        };\n\n        // ... process audio ...\n\n        DataCallbackResult::Continue\n    }\n}\n```\n\n**User Impact**: Temporary silence in onset detection (some sounds not detected), but no crash. Logged for debugging.\n\n## Testing Strategy\n\n### Unit Testing\n\n**Rust Unit Tests** (inline in modules):\n\n**File**: `rust/src/analysis/features.rs`\n```rust\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_spectral_centroid() {\n        let extractor = FeatureExtractor::new(48000);\n\n        // Synthetic low-frequency signal (kick-like)\n        let mut signal = vec![0.0f32; 1024];\n        for i in 0..1024 {\n            signal[i] = (2.0 * std::f32::consts::PI * 100.0 * i as f32 / 48000.0).sin();\n        }\n\n        let features = extractor.extract(&signal);\n        assert!(features.centroid < 500.0, \"Expected low centroid for 100Hz sine wave\");\n    }\n\n    #[test]\n    fn test_zcr_calculation() {\n        // Noisy signal (hi-hat-like)\n        let signal: Vec<f32> = (0..1024).map(|_| rand::random::<f32>() * 2.0 - 1.0).collect();\n\n        let extractor = FeatureExtractor::new(48000);\n        let features = extractor.extract(&signal);\n\n        assert!(features.zcr > 0.3, \"Expected high ZCR for noise\");\n    }\n}\n```\n\n**File**: `rust/src/analysis/quantizer.rs`\n```rust\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use std::sync::{Arc, atomic::{AtomicU64, AtomicU32, Ordering}};\n\n    #[test]\n    fn test_on_time_detection() {\n        let frame_counter = Arc::new(AtomicU64::new(0));\n        let bpm = Arc::new(AtomicU32::new(120));\n        let quantizer = Quantizer::new(frame_counter.clone(), bpm, 48000);\n\n        // 120 BPM = 0.5 sec/beat = 24000 samples/beat at 48kHz\n        let onset_timestamp = 24000;  // Exactly on beat 1\n\n        let feedback = quantizer.quantize(onset_timestamp);\n        assert_eq!(feedback.classification, TimingClassification::OnTime);\n        assert!(feedback.error_ms.abs() < 1.0, \"Error should be near 0ms\");\n    }\n\n    #[test]\n    fn test_late_detection() {\n        let frame_counter = Arc::new(AtomicU64::new(0));\n        let bpm = Arc::new(AtomicU32::new(120));\n        let quantizer = Quantizer::new(frame_counter, bpm, 48000);\n\n        // Onset at 100ms after beat (late)\n        let onset_timestamp = 24000 + 4800;  // 24000 samples + 100ms\n\n        let feedback = quantizer.quantize(onset_timestamp);\n        assert_eq!(feedback.classification, TimingClassification::Late);\n        assert!((feedback.error_ms - 100.0).abs() < 1.0);\n    }\n}\n```\n\n**Dart Widget Tests**:\n\n**File**: `test/ui/widgets/classification_indicator_test.dart`\n```dart\nimport 'package:flutter_test/flutter_test.dart';\nimport 'package:beatbox_trainer/ui/widgets/classification_indicator.dart';\n\nvoid main() {\n  testWidgets('ClassificationIndicator shows KICK in red', (tester) async {\n    await tester.pumpWidget(\n      MaterialApp(\n        home: ClassificationIndicator(\n          result: ClassificationResult(\n            sound: BeatboxHit.kick,\n            timing: TimingFeedback(classification: TimingClassification.onTime, errorMs: 0.0),\n            timestampMs: 0,\n          ),\n        ),\n      ),\n    );\n\n    expect(find.text('KICK'), findsOneWidget);\n\n    final container = tester.widget<Container>(find.byType(Container));\n    expect(container.color, Colors.red);\n  });\n}\n```\n\n### Integration Testing\n\n**Audio Loopback Test** (manual, real device):\n\n**Objective**: Measure end-to-end latency using audio loopback cable (3.5mm cable connecting headphone jack to microphone).\n\n**Procedure**:\n1. Connect loopback cable\n2. Start app, set BPM to 60 (1 beat/sec)\n3. Record metronome click output via loopback\n4. Measure time from click generation to onset detection\n5. Verify latency < 20ms\n\n**Pass Criteria**:\n- 95% of onsets detected within 20ms of click\n- 0% xruns (buffer underruns) during 60-second test\n\n**Jitter Measurement Test** (manual):\n\n**Objective**: Verify metronome has 0 jitter.\n\n**Procedure**:\n1. Record metronome output to WAV file (60 seconds at 120 BPM)\n2. Analyze in DAW (Audacity, Ableton) with sample-level precision\n3. Measure inter-click intervals\n4. Calculate jitter as standard deviation of intervals\n\n**Pass Criteria**:\n- All intervals = exactly 24000 samples (at 48kHz, 120 BPM)\n- Jitter = 0 samples (standard deviation = 0)\n\n**Calibration Accuracy Test** (manual, real users):\n\n**Objective**: Verify calibration improves classification accuracy.\n\n**Procedure**:\n1. Recruit 5 testers with different voices\n2. Each tester calibrates app (30 samples)\n3. Each tester performs 100 test sounds (33 kick, 33 snare, 34 hi-hat)\n4. Record classification accuracy\n\n**Pass Criteria**:\n- Mean accuracy > 90% across all testers\n- No tester has accuracy < 80%\n\n### End-to-End Testing\n\n**User Scenario 1: First-Time User Calibration**\n\n**Steps**:\n1. Launch app\n2. Tap \"Calibrate\"\n3. Follow on-screen instructions for 30 samples\n4. Verify calibration completes successfully\n5. Tap \"Start Training\"\n6. Verify metronome plays\n7. Make 10 beatbox sounds\n8. Verify classification and timing feedback appears\n\n**Pass Criteria**:\n- Calibration completes in < 2 minutes\n- No crashes or errors\n- All 10 sounds detected and classified\n\n**User Scenario 2: Training Session with BPM Changes**\n\n**Steps**:\n1. Start training at 80 BPM\n2. Practice for 30 seconds\n3. Change BPM to 140\n4. Practice for 30 seconds\n5. Verify metronome adjusts immediately\n\n**Pass Criteria**:\n- BPM change takes effect within 1 second\n- No audio glitches during transition\n- Timing feedback remains accurate at both BPMs\n",
  "fileStats": {
    "size": 37040,
    "lines": 1220,
    "lastModified": "2025-11-12T15:26:29.596Z"
  },
  "comments": []
}