{
  "id": "snapshot_1762959622095_mjrmc4bon",
  "approvalId": "approval_1762959571591_pvqxisrgv",
  "approvalTitle": "Technology Stack Document",
  "version": 2,
  "timestamp": "2025-11-12T15:00:22.095Z",
  "trigger": "approved",
  "status": "pending",
  "content": "# Technology Stack\n\n## Project Type\nReal-time Android mobile application with native audio processing capabilities and cross-platform UI framework.\n\n## Core Technologies\n\n### Primary Language(s)\n- **Dart 3.x**: Flutter UI layer\n- **Rust**: Real-time audio analysis engine (DSP processing, feature extraction, classification)\n- **C++**: Low-latency audio I/O via Oboe library\n- **Java/Kotlin**: Android integration layer (JNI bridge, MainActivity)\n\n**Language-specific tools**:\n- `cargo`: Rust package manager and build tool\n- `flutter`: Dart SDK and Flutter framework tooling\n- `gradle`: Android build system\n- `rustc`: Rust compiler with Android NDK target support\n\n### Key Dependencies/Libraries\n\n**Audio I/O & Real-time Processing**:\n- **oboe-rs** (v0.6+): Rust bindings for Google's C++ Oboe library - provides low-latency, full-duplex audio streams with sample-accurate timing\n- **rtrb** (v0.3+): Lock-free, wait-free SPSC (Single Producer Single Consumer) ring buffer for zero-allocation audio thread communication\n\n**DSP & Signal Processing**:\n- **rustfft** (v6+): High-performance FFT implementation for spectral analysis\n- **microfft** (v0.5+): Optimized real-valued FFT for onset detection (alternative to rustfft for smaller window sizes)\n- **aus** or **estratto**: Spectral feature extraction (centroid, flatness, rolloff, ZCR)\n- **microdsp** or **aubio-rs**: Spectral flux onset detection algorithms\n\n**Cross-Language Bridge**:\n- **flutter_rust_bridge** (v2+): Automated FFI/JNI code generation for Dart ↔ Rust communication with async Stream support\n- **jni** (v0.21+): JNI bindings for Rust\n- **ndk-context** (v0.1+): Android context initialization for native libraries in Flutter apps\n\n**Android Integration**:\n- **Flutter SDK** (v3.19+): Cross-platform UI framework\n- **Android NDK**: Native development kit for compiling Rust/C++ to ARM/ARM64 targets\n\n### Application Architecture\n\n**4-Layer Native-First Stack**:\n\n```\n┌─────────────────────────────────────┐\n│  Layer 4: Dart/Flutter UI           │  ← User interaction, visualization\n├─────────────────────────────────────┤\n│  Layer 3: flutter_rust_bridge       │  ← Type-safe FFI/JNI bridge\n├─────────────────────────────────────┤\n│  Layer 2: Rust Audio Engine         │  ← DSP analysis, classification, state\n├─────────────────────────────────────┤\n│  Layer 1: C++ Oboe (via oboe-rs)    │  ← Low-latency audio I/O\n└─────────────────────────────────────┘\n```\n\n**Architectural Pattern**: Event-driven, lock-free multi-threaded pipeline\n\n**Core Threads**:\n1. **AudioThread (Real-time)**: Oboe output callback (master), drives audio I/O at hardware clock rate, generates metronome clicks, performs non-blocking microphone reads\n2. **AnalysisThread (Non-real-time)**: Consumes audio data from lock-free queue, executes DSP algorithms (FFT, onset detection, feature extraction, classification)\n3. **UI Thread (Flutter)**: Receives classification results via flutter_rust_bridge Stream, updates visual feedback\n\n**Critical Design Principles**:\n- **Zero allocation in audio path**: Pre-allocated buffer pool pattern with dual SPSC queues (DATA_QUEUE + POOL_QUEUE)\n- **Lock-free communication**: `rtrb` ring buffers eliminate mutex contention\n- **Full-duplex synchronization**: Output stream (metronome) is master, input stream (microphone) is slave to prevent drift\n- **Sample-accurate timing**: Metronome generated in audio callback at frame granularity (0 jitter)\n\n### Data Storage\n- **Primary storage**: In-memory state (no persistent database in v1)\n- **Caching**: Calibration thresholds stored in Rust state (per-user thresholds for centroid, ZCR, etc.)\n- **Data formats**:\n  - Audio: Raw f32 PCM samples (32-bit floating point)\n  - Configuration: Rust structs serialized via flutter_rust_bridge\n  - Calibration profiles: Future enhancement - JSON export/import\n\n### External Integrations\n- **APIs**: None (fully offline application)\n- **Protocols**: JNI for Java ↔ Rust communication, FFI for Dart ↔ Rust communication\n- **Authentication**: N/A (local-only app)\n\n### Monitoring & Dashboard Technologies\n- **Dashboard Framework**: Flutter (Dart) - Material Design widgets\n- **Real-time Communication**: `flutter_rust_bridge` Stream from Rust → Dart (push-based, async)\n- **Visualization Libraries**: Flutter CustomPainter for waveform rendering (future enhancement)\n- **State Management**: StreamBuilder pattern for reactive UI updates\n\n## Development Environment\n\n### Build & Development Tools\n- **Build System**:\n  - Flutter build system (wraps Gradle for Android)\n  - Cargo for Rust library compilation\n  - Android NDK integration via `cargo-ndk` or manual cross-compilation\n- **Package Management**:\n  - `pub` (Dart/Flutter dependencies)\n  - `cargo` (Rust crates)\n  - `gradle` (Android dependencies)\n- **Development workflow**:\n  - Flutter hot reload for UI iteration\n  - Rust: `cargo watch` for native library recompilation\n  - `flutter_rust_bridge_codegen` for regenerating FFI bindings on API changes\n\n### Code Quality Tools\n- **Static Analysis**:\n  - `dart analyze` (Dart linter)\n  - `clippy` (Rust linter with strict warnings)\n- **Formatting**:\n  - `dart format` (automatic Dart formatting)\n  - `rustfmt` (automatic Rust formatting per project style guide)\n- **Testing Framework**:\n  - Dart: `flutter_test` for widget and integration tests\n  - Rust: `cargo test` for unit tests (DSP algorithm correctness, calibration logic)\n  - Manual: Real-device audio latency profiling with audio loopback cable\n- **Documentation**:\n  - `dartdoc` for Dart API documentation\n  - `cargo doc` for Rust module documentation\n\n### Version Control & Collaboration\n- **VCS**: Git\n- **Branching Strategy**: GitHub Flow (feature branches → main)\n- **Code Review Process**: Pull requests required for main branch, focus on:\n  - Real-time safety (no allocations/locks in audio callbacks)\n  - Numeric correctness of DSP algorithms\n  - JNI initialization integrity\n\n## Deployment & Distribution\n- **Target Platform(s)**: Android 7.0+ (API level 24+), ARM64-v8a and armeabi-v7a architectures\n- **Distribution Method**:\n  - Google Play Store (release builds)\n  - APK direct download for testing (debug/profile builds)\n- **Installation Requirements**:\n  - Microphone and speaker/headphone access\n  - Minimum 2GB RAM (for FFT processing)\n  - Android device with low-latency audio support (check via `AudioManager.getProperty(PROPERTY_OUTPUT_FRAMES_PER_BUFFER)`)\n- **Update Mechanism**: Standard Google Play Store auto-update flow\n\n## Technical Requirements & Constraints\n\n### Performance Requirements\n- **End-to-end audio latency**: < 20ms target (10-15ms achievable with Oboe double-buffering on modern devices)\n- **Metronome jitter**: 0 samples (sample-accurate generation eliminates timing drift)\n- **Classification latency**: < 100ms from onset detection to UI feedback\n- **CPU usage**: < 15% sustained on mid-range device (Snapdragon 660-class SoC)\n- **Memory footprint**: < 100MB total, < 5MB for audio buffers\n- **App size**: < 50MB (no ML models, pure DSP)\n\n**Benchmarks**:\n- FFT computation (1024-point): < 2ms on ARM Cortex-A73\n- Feature extraction (5 features): < 1ms\n- Onset detection (256-sample window, spectral flux): < 0.5ms\n\n### Compatibility Requirements\n- **Platform Support**: Android 7.0+ (API 24+)\n- **Architectures**: ARM64-v8a (primary), armeabi-v7a (fallback)\n- **Dependency Versions**:\n  - Flutter SDK: 3.19+\n  - Rust toolchain: 1.75+ with `aarch64-linux-android` and `armv7-linux-androideabi` targets\n  - Android NDK: r25c or later\n- **Standards Compliance**:\n  - Android Audio API best practices (Oboe recommended guidelines)\n  - No reliance on deprecated AudioTrack/AudioRecord APIs\n\n### Security & Compliance\n- **Security Requirements**:\n  - Microphone permission requested at runtime (Android 6.0+ permission model)\n  - No network access required or requested\n  - No data collection or telemetry\n- **Compliance Standards**: N/A (single-user, offline, non-commercial training tool)\n- **Threat Model**:\n  - Low-risk surface (no network, no persistent storage, no user data)\n  - Memory safety via Rust (prevents buffer overflows in DSP code)\n\n### Scalability & Reliability\n- **Expected Load**: Single user per device, no concurrent sessions\n- **Availability Requirements**: Offline-first (100% availability with no network dependency)\n- **Growth Projections**:\n  - Phase 1: Support 3 sound categories (kick, snare, hi-hat)\n  - Phase 2: Extend to 8+ categories (rim shot, cymbal, throat bass, etc.)\n  - Phase 3: User-defined custom sounds via template matching\n\n## Technical Decisions & Rationale\n\n### Decision Log\n\n1. **Native-First Stack (C++ Oboe → Rust → Java → Dart) over High-Level Plugins**:\n   - **Rationale**: High-level Dart audio plugins (e.g., `flutter_sound`, `just_audio`) introduce unacceptable latency (50-200ms) and jitter due to:\n     - Garbage collection pauses in Dart VM\n     - Channel bridging overhead (platform channels add 10-30ms latency)\n     - Inability to control buffer sizes and audio thread priority\n   - **Trade-offs**: Increased complexity (4-layer stack, manual JNI/FFI setup) in exchange for deterministic real-time performance\n   - **Validation**: Oboe documentation confirms < 20ms latency achievable only via native APIs\n\n2. **Heuristic DSP over Machine Learning for Sound Classification**:\n   - **Rationale**:\n     - **Interpretability**: Users understand \"brightness\" (spectral centroid) and \"noisiness\" (ZCR) as tangible concepts vs. black-box ML predictions\n     - **Calibration**: Threshold-based rules adapt to individual users via simple 2-minute calibration (10 samples per category)\n     - **Resource efficiency**: No model loading, no inference overhead, < 5MB code footprint\n     - **Progressive difficulty**: Rule complexity scales programmatically (Level 1: 2 features, Level 2: 5+ features) without retraining\n   - **Trade-offs**: Lower initial accuracy for uncalibrated users, manual feature engineering required\n   - **Alternatives considered**: TensorFlow Lite (rejected due to 20-50MB model size, 20ms inference latency, lack of interpretability)\n\n3. **Lock-Free SPSC Queues (rtrb) over Mutexes**:\n   - **Rationale**: Audio callbacks execute in high-priority real-time thread - any blocking (mutex contention) causes audible glitches (\"xruns\")\n   - **Implementation**: Dual-queue \"object pool\" pattern with pre-allocated buffers eliminates heap allocations in audio path\n   - **Trade-offs**: More complex buffer lifecycle management vs. guaranteed deterministic execution time\n   - **Validation**: Real-time audio literature universally mandates lock-free communication for audio threads\n\n4. **Full-Duplex Master-Slave Synchronization over Independent Streams**:\n   - **Rationale**: Independent input/output streams drift apart due to different hardware clocks, making timing quantization impossible\n   - **Implementation**: Output stream (metronome) is master, input stream (microphone) reads are triggered from output callback\n   - **Trade-offs**: Slightly more complex initialization logic vs. eliminating unbounded clock drift\n   - **Source**: Oboe team's FullDuplexStream pattern (recommended best practice)\n\n5. **Multi-Resolution STFT Strategy over Single FFT Window Size**:\n   - **Rationale**: Time-frequency uncertainty principle - optimal onset detection requires small windows (256 samples), optimal classification requires large windows (1024 samples)\n   - **Implementation**:\n     - Pipeline 1 (continuous): 256-sample FFT for spectral flux onset detection\n     - Pipeline 2 (event-triggered): 1024-sample FFT for feature extraction at detected onsets\n   - **Trade-offs**: Dual FFT computation increases CPU usage by ~30% vs. achieving both accurate timing and accurate classification\n   - **Alternatives rejected**: Single 512-sample compromise window results in \"blurred\" onsets and \"blurred\" spectral features\n\n6. **Sample-Accurate Metronome over Dart Timer.periodic**:\n   - **Rationale**: Dart/Flutter timers exhibit 50-100ms jitter (documented in `Timer` API and `audioPlayers` plugin issues), making rhythm training impossible\n   - **Implementation**: Metronome clicks synthesized directly in Oboe output callback using modulo arithmetic on frame counter\n   - **Trade-offs**: Zero jitter vs. slightly more complex audio generation logic\n   - **Validation**: Sample-accurate generation eliminates all timing measurement ambiguity\n\n7. **Manual JNI_OnLoad Implementation over Automatic Initialization**:\n   - **Rationale**: Flutter apps load native libraries via Dart FFI, not via Java `System.loadLibrary()`, causing `ndk_context` to remain uninitialized\n   - **Implementation**: Manual `System.loadLibrary()` in Kotlin `MainActivity.init` block + explicit `JNI_OnLoad` function in Rust\n   - **Trade-offs**: 30 lines of boilerplate code vs. preventing \"android context was not initialized\" crash on app launch\n   - **Source**: Critical undocumented requirement for using oboe-rs within Flutter (discovered via community forums)\n\n## Known Limitations\n\n- **Single-user calibration**: Calibration profiles are not shareable between users - each user must complete 2-minute initial setup\n  - **Impact**: Onboarding friction for new users\n  - **Future solution**: Community-contributed \"voice profile presets\" (e.g., \"deep male voice\", \"high female voice\")\n\n- **Device audio latency variability**: Some low-end Android devices report 40-60ms latency even with Oboe optimizations\n  - **Impact**: Training effectiveness reduced on budget hardware (< $150 devices)\n  - **Workaround**: Display device latency estimate in settings, recommend users test with audio loopback\n\n- **No background operation**: Android audio focus and power management prevent training during screen-off\n  - **Impact**: Users cannot practice with screen off to save battery\n  - **When addressed**: Phase 2 with wake lock and audio focus management\n\n- **Fixed BPM during session**: Changing BPM requires stopping and restarting the audio engine\n  - **Impact**: Cannot practice tempo ramping (gradual BPM increase)\n  - **Why it exists**: Rust audio state is immutable during active callback loop for real-time safety\n  - **Future solution**: Lock-free command queue for control messages (BPM updates, start/stop)\n\n- **FFT bin resolution constraints**: 1024-sample FFT at 48kHz = ~47Hz per bin, insufficient for distinguishing sub-bass nuances (40Hz kick vs. 60Hz kick)\n  - **Impact**: Very low-frequency sounds may be misclassified\n  - **Mitigation**: Calibration step accounts for individual \"kick\" frequency range\n  - **Future enhancement**: Zero-padding or chirp-Z transform for higher frequency resolution in bass region\n\n- **No session history persistence**: Practice statistics (accuracy over time, weak spots) reset on app restart\n  - **Impact**: Users cannot track long-term progress\n  - **When addressed**: Phase 3 with local SQLite database and analytics dashboard\n",
  "fileStats": {
    "size": 15315,
    "lines": 263,
    "lastModified": "2025-11-12T14:59:17.533Z"
  },
  "comments": []
}